import sys
import subprocess
from typing import List

# Default configuration constants
DEFAULT_LLM = "llama2:latest"
ALTERNATE_LLM = "moondream"
DEFAULT_TEMPERATURE = 0.9
DEFAULT_MAX_TOKEN = 100

# Constant for buffering size
DEFAULT_BUFFER_SIZE = 1


class OllamaModel:
    """
    Represents a custom model for text generation using the Ollama tool.

    This class provides functionality to initialize a customizable text generation model, create
    the model using the `ollama create` command, and execute AI-driven prompts by interfacing
    with the Ollama runtime.

    :ivar model_name: Name of the custom model to be used or generated.
    :type model_name: str
    :ivar temperature: Temperature setting used for text generation, controlling randomness.
    :type temperature: float
    :ivar max_token: Maximum number of tokens to be generated by the model per output.
    :type max_token: int
    """
    def __init__(self, model_name: str = DEFAULT_LLM, temperature: float = DEFAULT_TEMPERATURE,
                 max_token: int = DEFAULT_MAX_TOKEN):
        """
        Initialize the class with the model name and configuration parameters.
        :param model_name: Custom model name.
        :param temperature: Temperature used for text generation.
        :param max_token: Maximum number of tokens to generate.
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_token = max_token

    def _build_run_command(self) -> List[str]:
        """
        Build the command for running the `ollama run` model.
        :return: List of arguments for the command.
        """
        return ["ollama", "run", self.model_name]

    def create_model(self):
        """
        Create a custom model using the `ollama create` command.
        """
        try:
            print(f"Generating model: '{self.model_name}'...")
            subprocess.run(["ollama", "create", self.model_name], check=True)
            print(f"Model '{self.model_name}' was successfully generated.")
        except subprocess.CalledProcessError as create_error:
            print(f"Error while generating the model: {create_error}")

    def execute_prompt(self, prompt: str) -> str:
        """
        Execute the given prompt using the custom model, reading the output character by character.
        :param prompt: Text to be sent to the model.
        :return: Response generated by the model.
        """
        try:
            # Build the command for the model
            command = self._build_run_command()
            process = subprocess.Popen(
                command,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=DEFAULT_BUFFER_SIZE  # Use predefined buffer size
            )

            # Send the prompt to the command via stdin
            process.stdin.write(prompt)
            process.stdin.close()

            output = []
            # Read output character by character in real-time
            for char in iter(lambda: process.stdout.read(1), ''):
                if char:  # If the character is valid, print it
                    sys.stdout.write(char)
                    sys.stdout.flush()
                    output.append(char)

            process.wait()  # Wait for the command to finish
            return "".join(output).strip()

        except subprocess.CalledProcessError as command_error:
            print("Error occurred during the prompt execution:")
            print(command_error.stderr)
            return f"Error occurred during the prompt execution: {command_error}"
        except Exception as generic_error:
            print(f"Unexpected error: {generic_error}")
            return f"Unexpected error: {generic_error}"

llm = OllamaModel()

if __name__ == "__main__":
    # Create an instance of OllamaModel
    model = llm

    # Optionally, create the model (if needed)
    # model.create_model()

    # Prompt the model with a sample input
    prompt_text = "Tell me this word 20 times: hello"
    response = model.execute_prompt(prompt_text)

    # Print the final response (uncomment to see output)
    # print("\nFinal response:", response)